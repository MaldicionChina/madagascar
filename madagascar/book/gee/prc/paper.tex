
\def\sx#1{}
\def\bx#1{#1}
\def\eq{\quad =\quad}

\title{Preconditioning}
\author{Jon Claerbout}
\maketitle

\label{paper:prc}
\sx{precondition}


\par
Let us review the big picture.
In Chapter \ref{paper:ajt} we developed adjoints and
in Chapter \ref{paper:lsq} we developed inverse operators.
Logically, correct solutions come only through inversion.
Real life, however, seems nearly the opposite.
This is puzzling but intriguing.
\par
Every time you fill your car with gasoline,
it derives much more from the adjoint than from inversion.
I refer to the fact that ``practical seismic data processing''
relates much more to the use of adjoints than of inverses.
It has been widely known for about the last 15 years
that medical imaging and all basic image creation methods are like this.
It might seem that an easy path to fame and profit would
be to introduce the notion of inversion,
but it is not that easy.
Both cost and result quality enter the picture.
\par
First consider cost.
For simplicity, consider a data space with $N$ values
and a model (or image) space of the same size.
The computational cost of applying a dense adjoint
operator increases in direct proportion to the number
of elements in the matrix, in this case $N^2$.
To achieve the minimum discrepancy between theoretical data
and observed data (inversion) theoretically requires $N$ iterations
raising the cost to $N^3$.
\par
Consider an image of size $m\times m=N$.
Continuing, for simplicity, to assume a dense matrix of relations between
model and data,
the cost for the adjoint is $m^4$ whereas the cost for inversion is $m^6$.
We'll consider computational costs for the year 2000, but
noticing that costs go as the sixth power of the mesh size,
the overall situation will not change much in the foreseeable future.
Suppose you give a stiff workout to a powerful machine;
you take an hour to invert a $4096\times 4096$ matrix.
The solution, a vector of $4096$ components could
be laid into an image of size $64\times 64= 2^6\times 2^6 = 4096$.
Here is what we are looking at for costs:
\par
\begin{center}
\begin{tabular}{||r|r|r|r|r||}                                          \hline
adjoint cost &$(m\times m )^2$ & $(512\times 512)^2$ & $(2^9 2^9)^2$ & $2^{36}$
\\ \hline
inverse cost &$(m\times m )^3$ & $  (64\times 64)^3$ & $(2^6 2^6)^3$ & $2^{36}$
\\ \hline
\end{tabular}
\end{center}
\par\noindent
These numbers tell us that for applications with dense operators,
the biggest images that we are likely to see coming from inversion methods
are $64\times 64$ whereas those from adjoint methods are $512\times 512$.
For comparison, the retina of your eye is comparable to your computer
screen at $1000\times 1000$.
We might summarize by saying that while adjoint methods are less than perfect,
inverse methods are ``legally blind'' :-)
\sideplot{512x512}{height=3.5in,width=3.5in}{
	Jos greets Andrew, ``Welcome back Andrew''
	from the Peace Corps.
	At a resolution of $512\times 512$, this picture
	is about the same as the resolution
	as the paper it is printed on,
	or the same as your viewing screen,
	if you have scaled it to 50\%\ of screen size.}
%\newslide

\par\noindent
http://sepwww.stanford.edu/sep/jon/family/jos/gifmovie.html holds a movie
blinking between Figures \ref{fig:512x512} and \ref{fig:64x64}.
\par
This cost analysis is oversimplified in that most applications
do not require dense operators.
With sparse operators, the cost advantage of adjoints is even more
pronounced since for adjoints,
the cost savings of operator sparseness translate directly to
real cost savings.
The situation is less favorable and much more muddy for inversion.
The reason that Chapter 2 covers iterative methods
and neglects exact methods is that in practice
iterative methods are not run to their theoretical completion
but they run until we run out of patience.

\par
Cost is a big part of the story, but the story has many other parts.
Inversion, while being the only logical path to the best answer,
is a path littered with pitfalls.
The first pitfall is that the data is rarely able to 
determine a complete solution reliably.
Generally there are aspects of the image that are not learnable
from the data.

\sideplot{64x64}{height=3.5in,width=3.5in}{
	Jos greets Andrew, ``Welcome back Andrew'' again.
	At a resolution of $64\times 64$
	the pixels are clearly visible.
	From far the pictures are the same.
	From near, examine their glasses.}
%\newslide


When I first realized that practical imaging methods in widespread
industrial use amounted merely to the adjoint of forward modeling,
I (and others) thought an easy way to achieve fame and fortune
would be to introduce the first steps towards inversion
along the lines of Chapter \ref{paper:lsq}.
Although inversion generally requires a prohibitive number
of steps, I felt that moving in the gradient direction,
the direction of steepest descent, would move us rapidly
in the direction of practical improvements.
This turned out to be optimistic.
It was too slow.
But then I learned about the conjugate gradient method that
spectacularly overcomes a well-known speed problem with the
method of steepest descents.
I came to realize that it was still too slow.
I learned this by watching the convergence in Figure
\ref{fig:conv}.
This led me to the helix method in Chapter \ref{paper:hlx}.
Here we'll see how it speeds many applications.

\par
We'll also come to understand why the gradient is such a poor direction
both for steepest descent and for conjugate gradients.
An indication of our path is found in the contrast between
an exact solution
$\bold m = (\bold A\T\bold A)^{-1}\bold A\T\bold d$ and the
gradient
$\Delta \bold m = \bold A\T\bold d$
(which is the first step starting from $\bold m =\bold 0$).
Notice that $\Delta \bold m$ differs from $\bold m$
by the factor $(\bold A\T\bold A)^{-1}$.
This factor is sometimes called a spectrum
and in some situations it literally is a frequency spectrum.
In these cases, $\Delta \bold m$ simply gets a different
spectrum from $\bold m$ and many iterations are required to fix it.
Here we'll find that for many applications,
``preconditioning'' with the helix is a better way.



\section{PRECONDITIONED DATA FITTING}
\par
Iterative methods (like conjugate-directions) can sometimes be accelerated
by a \bx{change of variables}.
The simplest change of variable is called a ``trial solution''.
Formally, we write the solution as
\begin{equation}
\bold m \eq \bold S \bold p
\end{equation}
where $\bold m$ is the map we seek,
columns of the matrix $\bold S$ are ``shapes'' that we like,
and coefficients in $\bold p$ are unknown coefficients
to select amounts of the favored shapes.
The variables $\bold p$  are often called the ``preconditioned variables''.
It is not necessary that $\bold S$ be an invertible matrix,
but we'll see later that invert-ability is helpful.
Take this trial solution and insert it into
a typical fitting goal
\begin{equation}
\bold 0 \quad\approx\quad  \bold F \bold m \ -\  \bold d
\end{equation}
and get
\begin{equation}
\bold 0 \quad\approx\quad  \bold F \bold S \bold p \ -\  \bold d
\end{equation}
We pass the operator $\bold F \bold S$ to our iterative solver.
After finding the best fitting                                      $\bold p$,
we merely evaluate
                                                $ \bold m = \bold S \bold p$
to get the solution to the original problem.

\par
We hope this change of variables has saved effort.
For each iteration, there is a little more work:
Instead of the iterative application of
                                                $\bold F$ and $\bold F\T$
we have iterative application of
                                        $\bold F\bold S$ and $\bold S\T\bold F\T$.

Our hope is that the number of iterations decreases because we are clever,
or because we have been lucky in our choice of $\bold S$.
Hopefully,
the extra work of the preconditioner operator $\bold S$
is not large compared to $\bold F$.
If we should be so lucky that
$\bold S= \bold F^{-1}$,
then we get the solution immediately.
Obviously we would try any guess with
$\bold S\approx \bold F^{-1}$.
Where I have known such $\bold S$ matrices,
I have often found that convergence is accelerated,
but not by much.
Sometimes it is worth using $\bold F\bold S$ for a while in the beginning,
but later it is cheaper and faster to use only $\bold F$.
A practitioner might regard the guess of $\bold S$
as prior information,
like the guess of the initial model $\bold m_0$.

\par
For a square matrix $\bold S$,
the use of a preconditioner should not change the ultimate solution.
Taking $\bold S$ to be a tall rectangular matrix,
reduces the number of adjustable parameters,
changes the solution,
gets it quicker, but lower resolution.

\subsection{Preconditioner with a starting guess}

In many applications, for many reasons,
we have a starting guess $\bold m_0$ of the solution.
You might worry that
you could not find the starting preconditioned variable 
$\bold p_0= \bold S^{-1}\bold m_0$
because you did not know the inverse of $\bold S$.
The way to avoid this problem is to
reformulate the problem
in terms of a new variable $\tilde {\bold m}$
where
$ \bold m = \tilde {\bold m} + \bold m_0$.
Then 
$\bold 0\approx \bold F \bold m - \bold d$
becomes
$\bold 0\approx \bold F \tilde {\bold m} - (\bold d - \bold F \bold m_0)$
or
$\bold 0\approx \bold F \tilde {\bold m} - \tilde {\bold d}.$
Thus we have accomplished the goal of taking
a problem with a nonzero starting model
and converting it a problem of the same type
with a zero starting model.
Thus we do not need the inverse of $\bold S$
because the iteration starts from $\tilde {\bold m}=\bold 0$
so $\bold {\tilde p_0} = \bold 0$.
\par
Unfortunately, as we will see later,
this conclusion is only valid while there is no regularization.

\subsection{ Monkeying with the gradient }
An essential part of any model update is the gradient
\begin{equation}
\Delta \bold m = \partial \bold r / \partial \bold m\T = \bold F\T \bold r 
\end{equation}
which vanishes $\bold 0=\bold F\T \bold r$ at the ultimate solution.
\par
With the change of variables $\bold m = \bold S \bold p$ the gradient changes as
$\Delta \bold m = \bold S \Delta \bold p$.
In terms of $\bold p$ the regression
$\bold 0 \approx \bold r = \bold F \bold m - \bold d$
becomes
$\bold 0 \approx \bold r = \bold F \bold S \bold p - \bold d$.
As always, the gradient is simply the adjoint operator upon the residual.
$\Delta \bold p = \bold S\T \bold F\T \bold r$.
Changing variables back to $\bold m$ gives
\begin{equation}
\Delta \bold m = \bold S \Delta \bold p = \bold S \bold S\T \bold F\T \bold r
\end{equation}
The ultimate solution is when the gradient vanishes $\bold 0 = (\bold S\bold S\T) \bold F\T \bold r$.
Most often the ultimate solution is the same with or without the change of variables
because most often the transformation matrix $\bold S$
is chosen invertible so $ (\bold S\bold S\T)$ has an inverse which will cancel it.
\par
We conclude that when a gradient is multiplied by any
positive definite matrix $\bold S\bold S\T$
we are simply solving the original problem in a new coordinate system.
%In linear problems,
%we'd be doing the preconditioning (choosing the coordinate system)
%to maximize the speed of convergence.
%In nonlinear problems,
%as described in a later chapter,
%we are chosing a descent path that honors prior information
%hence is less likely to fall into a local minimum.
%\par
Choosing $\bold S\bold S\T$
really matters only when the problem is big so it converges slowly,
or when the physics or statistical approach is non-linear (later chapter).
The choice of $\bold S$ is a subjective matter.
It's an area where prior information or your physical intuition is relevant.
Since both $\bold m$ and $\bold p$ are in model space,
obvious choices for $\bold S$ are weighting functions
and filtering functions in model space.
You would weight the model space smaller where your intuition
tells you your ultimate model will be small or where it might not be
learnable from the data.
\par
In nonlinear problems, monkeying with the gradient modifies the descent path
in a way that honors prior information
hence this modification may be less likely to fall into a local minimum.




\section{PRECONDITIONING THE REGULARIZATION}

%\subsection{Recasting a fitting problem in white variables}

\par
The basic formulation of a geophysical estimation problem
consists of setting up
{\em  two}
goals,
one for data fitting,
and the other for model shaping.
With two goals, preconditioning is somewhat different.
The two goals may be written as:
\begin{eqnarray}
\bold 0 &\approx& \bold F \bold m - \bold d \\
\bold 0 &\approx& \bold A \bold m
\end{eqnarray}
which defines two residuals,
a so-called ``data residual'' and a ``model residual'' that
are usually minimized by conjugate-gradient, least-squares methods.
\par
To fix ideas, let us examine a toy example.
The data and the first three rows of the matrix below
are random numbers truncated to integers.
The model roughening operator $\bold A$
is a first differencing operator times 100.

%\newslide
{\samepage
\par\noindent
\footnotesize
\begin{verbatim}
d(m)     F(m,n)                                            iter  Norm
---     ------------------------------------------------   ---- -----------
 41.    -55. -90. -24. -13. -73.  61. -27. -19.  23. -55.     1 20.00396538
 33.      8. -86.  72.  87. -41.  -3. -29.  29. -66.  50.     2 12.14780140
-58.     84. -49.  80.  44. -52. -51.   8.  86.  77.  50.     3  8.94393635
  0.    100.   0.   0.   0.   0.   0.   0.   0.   0.   0.     4  6.04517126
  0.   -100. 100.   0.   0.   0.   0.   0.   0.   0.   0.     5  2.64737511
  0.      0.-100. 100.   0.   0.   0.   0.   0.   0.   0.     6  0.79238468
  0.      0.   0.-100. 100.   0.   0.   0.   0.   0.   0.     7  0.46083349
  0.      0.   0.   0.-100. 100.   0.   0.   0.   0.   0.     8  0.08301232
  0.      0.   0.   0.   0.-100. 100.   0.   0.   0.   0.     9  0.00542009
  0.      0.   0.   0.   0.   0.-100. 100.   0.   0.   0.    10  0.00000565
  0.      0.   0.   0.   0.   0.   0.-100. 100.   0.   0.    11  0.00000026
  0.      0.   0.   0.   0.   0.   0.   0.-100. 100.   0.    12  0.00000012
  0.      0.   0.   0.   0.   0.   0.   0.   0.-100. 100.    13  0.00000000
\end{verbatim}
}
\normalsize

\par
Notice at the tenth iteration,
the residual suddenly plunges 4 significant digits.
Since there are ten unknowns and the matrix is obviously full-rank,
conjugate-gradient theory tells us to expect
the exact solution at the tenth iteration.
This is the first miracle of conjugate gradients.
(The residual actually does not drop to zero.
What is printed in the \texttt{Norm} column
is the square root of the sum of the squares
of the residual components at the \texttt{iter}-th
iteration minus that at the last interation.)


\subsection{The second miracle of conjugate gradients}

The second miracle of conjugate gradients is exhibited below.
The data and data fitting matrix are the same,
but the model damping is simplified.

%\newslide
\par\noindent
\footnotesize
\begin{verbatim}
d(m)    F(m,n)                                            iter  Norm
---    ------------------------------------------------   ----  ----------
 41.   -55. -90. -24. -13. -73.  61. -27. -19.  23. -55.     1  3.64410686
 33.     8. -86.  72.  87. -41.  -3. -29.  29. -66.  50.     2  0.31269890
-58.    84. -49.  80.  44. -52. -51.   8.  86.  77.  50.     3 -0.00000021
  0.   100.   0.   0.   0.   0.   0.   0.   0.   0.   0.     4 -0.00000066
  0.     0. 100.   0.   0.   0.   0.   0.   0.   0.   0.     5 -0.00000080
  0.     0.   0. 100.   0.   0.   0.   0.   0.   0.   0.     6 -0.00000065
  0.     0.   0.   0. 100.   0.   0.   0.   0.   0.   0.     7 -0.00000088
  0.     0.   0.   0.   0. 100.   0.   0.   0.   0.   0.     8 -0.00000074
  0.     0.   0.   0.   0.   0. 100.   0.   0.   0.   0.     9 -0.00000035
  0.     0.   0.   0.   0.   0.   0. 100.   0.   0.   0.    10 -0.00000037
  0.     0.   0.   0.   0.   0.   0.   0. 100.   0.   0.    11 -0.00000018
  0.     0.   0.   0.   0.   0.   0.   0.   0. 100.   0.    12  0.00000000
  0.     0.   0.   0.   0.   0.   0.   0.   0.   0. 100.    13  0.00000000
\end{verbatim}
\normalsize
\par\noindent
Even though the matrix is full-rank,
we see the residual drop about 6 decimal places after the third iteration!
This convergence behavior is well known
in the computational mathematics literature.
Despite its practical importance,
it doesn't seem to have a name or identified discoverer.
So I call it the ``second miracle.''

\par
Practitioners usually don't like
the identity operator for model-shaping.
Generally they prefer to penalize wiggliness.
For practitioners,
the lesson of the second miracle of conjugate gradients
is that we have a choice of many iterations,
or learning to transform
independent variables so that
the regularization operator becomes an identity matrix.
Basically, such a transformation reduces the iteration count
from    something about the size of the model space
to      something about the size of the data space.
Such a transformation is called preconditioning.
In practice, data is often accumulated in bins.
Then the iteration count is reduced (in principle)
to the count of full bins
and should be independent of the count of the empty bins.
This allows refining the bins, enhancing the resolution.


\par
More generally,
the model goal $\bold 0 \approx \bold A \bold m$
introduces a roughening operator like a gradient,
Laplacian
(and in chapter \ref{paper:mda}
a Prediction-Error Filter (PEF)).
Thus the model goal is usually a filter,
unlike the data-fitting goal
which involves all manner of geometry and physics.
When the model goal is a filter its inverse is also a filter.
Of course this includes multidimensional filters with a helix.


\par
The preconditioning transformation
$\bold m = \bold S \bold p$
gives us
\begin{equation}
        \begin{array}{lll}
        \bold 0 &\approx & \bold F \bold S \bold p - \bold d \\
        \bold 0 &\approx & \bold A \bold S \bold p
        \label{eqn:invintprecond}
        \end{array}
\end{equation}
The operator $\bold A$ is a roughener while $\bold S$ is a smoother.
The choices of both $\bold A$ and $\bold S$ are somewhat subjective.
This suggests that we eliminate $\bold A$ altogether
by {\em  defining} it to be proportional to the inverse of $\bold S$,
thus $\bold A\bold S=\bold I$.
The fitting goals become
\begin{equation}
        \label{eqn:whitevar1}
        \begin{array}{lll}
        \bold 0 &\approx & \bold F  \bold S \bold p - \bold d \\
        \bold 0 &\approx & \epsilon\ \bold p
        \end{array}
\end{equation}
which enables us to benefit from the ``second miracle''.
After finding $\bold p$,
we obtain the final model with $\bold m = \bold S \bold p$.
\par
Please notice that the solution $\bold m$ is likely to come out smooth.
The solution for $\bold p$ will be more rough, because the
derivative of $m_t$ with respect to $t$ is $p_t$.
Because it is smooth, each $m_t$ is statistically correlated with its neighbors $m_{t+1}$ and $m_{t-1}$,
but the $p_t$ are less correlated with their neighbors.
That's a good thing, we soon see.


%Because model shaping is generally a filtering operation,
%and because preconditioning operators are best when
%they are invertible,

%\par
%As before (without regularization) we need a pair of
%roughening and smoothing operators that are mutually inverse
%$ \bold A\bold S=\bold I$.
%We need the roughener $\bold A$
%to get started with
%$\bold p_0=\bold A\bold m_0$,
%and we need the smoother $\bold S$ at each iteration.
%Besides Fourier transforms,
%the only known multidimensional roughening-smoothing operator pairs
%are the family of filters on a helix.
%Thus    deconvolution on a helix is an all-purpose
%        preconditioning strategy for multidimensional model regularization.


%\begin{notforlecture}
\subsection{Importance of scaling}

Another simple toy example shows us the importance of scaling.
We use the same example as above
except that the $i$-th column is multiplied by $i/10$
which means the $i$-th model variable has been divided by $i/10$.

%\newslide
\par\noindent
\footnotesize
\begin{verbatim}
d(m)    F(m,n)                                            iter  Norm
---    ------------------------------------------------   ---- -----------
 41.    -6. -18.  -7.  -5. -36.  37. -19. -15.  21. -55.     1 11.59544849
 33.     1. -17.  22.  35. -20.  -2. -20.  23. -59.  50.     2  6.97337770
-58.     8. -10.  24.  18. -26. -31.   6.  69.  69.  50.     3  5.64414406
  0.    10.   0.   0.   0.   0.   0.   0.   0.   0.   0.     4  4.32118177
  0.     0.  20.   0.   0.   0.   0.   0.   0.   0.   0.     5  2.64755201
  0.     0.   0.  30.   0.   0.   0.   0.   0.   0.   0.     6  2.01631355
  0.     0.   0.   0.  40.   0.   0.   0.   0.   0.   0.     7  1.23219979
  0.     0.   0.   0.   0.  50.   0.   0.   0.   0.   0.     8  0.36649203
  0.     0.   0.   0.   0.   0.  60.   0.   0.   0.   0.     9  0.28528941
  0.     0.   0.   0.   0.   0.   0.  70.   0.   0.   0.    10  0.06712411
  0.     0.   0.   0.   0.   0.   0.   0.  80.   0.   0.    11  0.00374284
  0.     0.   0.   0.   0.   0.   0.   0.   0.  90.   0.    12 -0.00000040
  0.     0.   0.   0.   0.   0.   0.   0.   0.   0. 100.    13  0.00000000
\end{verbatim}
\normalsize
We observe that solving the same problem for the scaled variables
has required a severe increase
in the number of iterations required to get the solution.
We lost the benefit of the second CG miracle.
Even the rapid convergence predicted for the 10-th iteration
is delayed until the 12-th.
%\end{notforlecture}

\begin{comment}

\subsection{Statistical interpretation}
This book is not a statistics book.
Never-the-less, many of you have some statistical knowledge
that allows you a statistical interpretation
of these views of preconditioning.
\par
A statistical concept is that we can combine many streams
of random numbers into a composite model.
Each stream of random numbers is generally taken
to be uncorrelated with the others,
to have zero mean, and to have the same variance
as all the others.
This is often abbreviated as IID, denoting
Independent, Identically Distributed.
Linear combinations
like filtering and weighting operations
of these IID random streams
can build correlated random functions much like those
observed in geophysics.
A geophysical practitioner seeks to do the inverse,
to operate on the correlated unequal random variables
and create the statistical ideal random streams.
The identity matrix required for the ``second miracle'',
and our search for a good preconditioning transformation
are related ideas.
The relationship will become more clear in chapter \ref{paper:mda}
when we learn how to estimate the best roughening operator $\bold A$
as a prediction-error filter.

\par
\boxit{
        Two philosophies to find a preconditioner:
        \begin{enumerate}
        \item
        Dream up a smoothing operator $\bold S$.
        \item
        Estimate a prediction-error filter $\bold A$,
        and then use its inverse $\bold S = \bold A^{-1}$.
        \end{enumerate}
        }


\par
\boxit{
        Deconvolution on a helix is an all-purpose
        preconditioning strategy for multidimensional model regularization.
        }


\par
The outstanding acceleration of convergence by preconditioning
suggests that the philosophy of image creation by optimization
has a dual orthonormality:
First, Gauss (and common sense) tells us that the data residuals
should be roughly equal in size.  Likewise in Fourier space
they should be roughly equal in size, which means they should
be roughly white, i.e. orthonormal.
(I use the word ``orthonormal''
because white means the autocorrelation is an impulse,
which means the signal is statistically orthogonal
to shifted versions of itself.)
Second,
to speed convergence of iterative methods,
we need a whiteness, another orthonormality, in the solution.
The map image, the physical function that we seek, might
not be itself white, so we should solve first for another variable,
the whitened map image, and as a final step,
transform it to the ``natural colored'' map.

\end{comment}

\subsection{You better make your residuals IID!}
In the statistical literature is a concept that repeatedly arises,
the idea that some statistical variables are IID, namely Independent, Identically Distributed.
In practice we'll see many random looking variables,
some much closer than others to IID.
Theoretically, the ID part of IID means the random variables come from Identical
probability Density functions.
In practice, the ID part mostly means the variables have the same variance.
The ``I'' before the ID means that the variables are statistically Independent of one another.
In the subject area of this book, signals, images, and earth volumes,
the ``I'' before the ID means that our residual spaces are white --
have all frequencies present in roughly equal amounts.
In other words the ``I'' means the statistical variables
have no significant correlation in time or space.
IID random variables have fairly uniform variance in both physical space and in Fourier space.
\boxit{
IID random variables have uniform variance in both physical space and Fourier space.
}

\par
In a geophysical project
it is important the residual between observed data and theoretical data is not far from IID.
We fit the data difference by minimizing the sum of the squared residuals,
so if any collection of residuals is small,
their squares are really small,
so such regression equations are effectively ignored.
We would hardly ever want that.
Consider reflection seismograms.   They get weak at late time.
So even with a bad fit the difference between real and theoretical seismograms
is necessarly weak at late times.
We don't want the data at late times to be ignored.
So we boost up the residual there.
We choose $\bold W$ to be a diagonal matrix that boosts late times
in the regression $\bold 0 \approx \bold r = \bold W(\bold F\bold m-\bold d)$
\par
An example with too much low (spatial) frequency in a residual might arise in a topographic study.
It is not unusual for the topographic wavelength to exceed the survey size.
Here we should choose $\bold W$ to be a filter to boost up the higher frequencies.
Perhaps $\bold W$ should contain a derivative or a Laplacian.
If you set up and solve a data modeling problem
and then find $\bold r$ is not IID,
you should consider changing your $\bold W$.
\par
Now let us include regularization $\bold 0 \approx\bold A\bold m$
and a preconditioning variable $\bold p$.
We have our data fitting goal and our model styling goal,
the first with a residual $\bold r_d$
in data space, the second with a residual $\bold r_m$
in model space.
We have had to choose a regularization operator $\bold A$ and a scaling factor $\epsilon$.
\begin{eqnarray}
0 & \approx& \bold r_d  \ = \ \bold F \bold A^{-1}\bold p -\bold d 
\\
0 & \approx& \bold r_m  \ = \ \epsilon \bold p
\end{eqnarray}
This system of two regressions could be packed into one;
the two residual vectors stacked on top of each other,
likewise the operators $\bold F$ and $\epsilon \bold I$.
The IID notion seems to apply to this unified system.
That gives us a clue
how we should have chosen the regularization operator $\bold A$.
Not only should $\bold r_d$ be IID, but also should $\bold r_m$.
But within a scale $\epsilon$, $\bold r_m=\bold p$.
Thus the preconditioning variable is not simply something to speed computational convergence.
It is a variable that should be IID.
If it is not coming out that way, we should consider changing $\bold A$.
Chapter \ref{paper:mda} addresses the task of choosing an $\bold A$
so that $\bold r_m$ comes out IID.
\par
\boxit{We should choose a weighting function (and/or operator) $\bold W$
so data residuals are IID.
We should also choose our regularization operator $\bold A$
so the precondioning variable $\bold p$ comes out IID.}

\subsection{Philosophical speculation}
\par
Finally, the $\epsilon$.  How should we choose this number?
Let ${\rm E}(|r_d|)$ be read as the Expected average value of $|r_d|$.
The concept that each component in the vector $\bold r = (\bold r_d,\bold r_m)\T$
should have the same expected absolute value leads to the notion that
the value of epsilon should be
$\epsilon={\rm E}(|r_d|)/{\rm E}(|p|)$.
(I vaguely remember trying this once and discovering that epsilon
could not be bootstrapped.  It either diverged to infinity
or converged to zero depending on its starting value.
Perhaps the epsilon we should use is the starting value poised between each divergence!
I don't trust my memory for an important issue like this.
Somebody else should try this again.)

\par
There is another strange idea here which is a consequence
of the notion that elements in  $\bold r = (\bold r_d,\bold r_m)\T$
should be IID.
It means elements of $\bold r_d$ and $\bold r_m$ should not be correlated.
``But wait'', you say, ``it makes no sense to correlate spaces of different dimension.''
That is where the formal statistical notion of ``ensemble'' arises.
If there are many worlds, and if we may speak of an average over worlds,
then we can have an array of averages,
the array being of dimension the number of components of $r_d$ by that of $r_m$
each an average over the many "worlds".
How can a practitioner absorb this notion?
Perhaps in some cases the model and data space have a natural alignment
such that a product of the two spaces can be locally averaged.
Post stack migration suggests the example
that a hyperbola in data space hitting its top in model space.
This suggests we use a $2\times 2$ weight matrix
\begin{equation}
\left[
\begin{array}{cc}
	W_{dd}  &  W_{dm}
	\\
	W_{dm}  &  W_{mm}
\end{array}
\right]
\end{equation}
The closest I have come to seeing something like this in practice is in
Chapter \ref{paper:noiz}
where two parts of model space,
the water bottom and the water top
came out correlated.
They should not have been so,
because their fluctuations had different mechanisms.


\subsection{The preconditioned solver}
Summing up the ideas above,
we start from fitting goals
\begin{equation}
\begin{array}{lll}
\label{eq:main}
\bold 0 &\approx& \bold F \bold m \ -\  \bold d \\
\bold 0 &\approx& \bold A \bold m
\end{array}
\end{equation}
and we change variables from
$\bold m$ to $\bold p$ using
$\bold m = \bold A^{-1} \bold p$
\begin{equation}
\begin{array}{llllcl}
\bold 0 &\approx &  \bold F \bold m \ -\  \bold d   &=&
    \bold F  \bold A^{-1} &\bold p  \ -\  \bold d
\\
\bold 0 &\approx &  \bold A \bold m       &=&   \bold I        & \bold p
\end{array}
\label{eqn:precsummary}
\end{equation}
Preconditioning means iteratively fitting
by adjusting the $\bold p$ variables
and then finding the model by using
$\bold m = \bold A^{-1} \bold p$.

\begin{comment}
A new reusable
preconditioned solver is
the module \texttt{solver_prc} \vpageref{lst:solver_prc}.
%The variable \texttt{x} in \texttt{prec\_solver} refers to $\bold m$.
Likewise the modeling operator $\bold F$ is called \texttt{Fop}
and the smoothing operator $\bold A^{-1}$ is called \texttt{Sop}.
Details of the code are only slightly different from
the regularized solver
\texttt{solver-reg} \vpageref{lst:solver-reg}.

%REMEMBER TO PUT THE PROGRAM BACK IN HERE !
%\begin{notforlecture}
\moddex{solver_prc}{Preconditioned solver}
%\end{notforlecture}
\end{comment}


\subsection{Need for an invertible preconditioner}
It is important to use regularization to solve many examples.
It is important to precondition because in practice computer power
is often a limiting factor.
It is important to be able to begin from a nonzero starting solution
because in nonlinear problems then we must restart from 
the result of an earlier solution.
Putting all three requirements together leads to a little problem.
It turns out the three together lead us to needing 
a preconditioning transformation that is invertible.
Let us see why this is so.

\begin{equation}
\begin{array}{lll}
\label{eq:main}
\bold 0 &\approx& \bold F \bold m \ -\  \bold d \\
\bold 0 &\approx& \bold A \bold m
\end{array}
\end{equation}
First we change variables from $\bold m$ to $\bold u = \bold m - \bold m_0$.
Clearly $\bold u$ starts from $\bold u_0=0$, and $\bold m = \bold u + \bold m_0$.
Then our regression pair becomes

\begin{equation}
\begin{array}{lll}
\label{eq:main}
\bold 0 &\approx& \bold F \bold u \ +\  (\bold F\bold m_0 -\bold d) \\
\bold 0 &\approx& \bold A \bold u \ +\  \bold A \bold m_0
\end{array}
\end{equation}
This result differs from the original regression in only two minor ways,
(1) revised data, and (2) a little more general form of the regularization,
the extra term $\bold A \bold m_0$.

Now let us introduce preconditioning.  From the regularization
we see this introduces the preconditioning variable $\bold p = \bold A\bold u$.
Our regression pair becomes:

\begin{equation}
\begin{array}{lll}
\label{eq:main}
\bold 0 &\approx& \bold F \bold A^{-1} \bold p \ +\  (\bold F\bold m_0 -\bold d) \\
\bold 0 &\approx&                      \bold p \ +\  \bold A \bold m_0
\end{array}
\end{equation}
Here is the problem:
Now we require both $\bold A$ and $\bold A^{-1}$ operators.
In 2- and 3-dimensional spaces we don't know very many operators
with an easy inverse.
Indeed, that is why I found myself pushed to come up with the helix methodology 
of the previous chapter -- because it provides invertible operators for
smoothing and roughening.





\section{OPPORTUNITIES FOR SMART DIRECTIONS}
Recall the fitting goals (\ref{eqn:precsummary})
\begin{equation}
\begin{array}{llllllcl}
\bold 0 &\approx& \bold r_d &=&  \bold F \bold m \ -\  \bold d   &=&
    \bold F  \bold A^{-1} &\bold p  \ -\  \bold d
    \\
\bold 0 &\approx& \bold r_m &=&  \bold A \bold m       &=&
    \bold I        & \bold p
\end{array}
\label{eqn:precsummary}
\end{equation}
Without preconditioning we have the search direction
\begin{equation}
\Delta \bold m_{\rm bad} \eq
\left[
	\begin{array}{cc}
	\bold F\T & \bold A\T
	\end{array}
\right]
\left[
	\begin{array}{c}
	\bold r_d \\
	\bold r_m
	\end{array}
\right]
\end{equation}
and with preconditioning we have the search direction
\begin{equation}
\Delta \bold p_{\rm good} \eq
\left[
	\begin{array}{cc}
	(\bold F\bold A^{-1})\T & \bold I
	\end{array}
\right]
\left[
	\begin{array}{c}
	\bold r_d \\
	\bold r_m
	\end{array}
\right]
\end{equation}
\par
The essential feature of preconditioning is not that we perform
the iterative optimization in terms of the variable $\bold p$.
The essential feature is that we use a search direction
that is a gradient with respect to $\bold p\T$ not $\bold m\T$.
Using $\bold A\bold m=\bold p$ we have
$\bold A\Delta \bold m=\Delta \bold p$.
This enables us to define a good search direction in model space.
\begin{equation}
\Delta \bold m_{\rm good} \eq \bold A^{-1}
\Delta \bold p_{\rm good} \eq
	\bold A^{-1} (\bold A^{-1})\T
	\bold F\T \bold r_d + \bold  A^{-1} \bold r_m
\end{equation}
Define the gradient by $\bold g=\bold F\T\bold r_d$ and
notice that $\bold r_m=\bold p$.
\begin{equation}
\Delta \bold m_{\rm good} \eq
	\bold A^{-1} (\bold A^{-1})\T \ \bold g
%	\frac{ \bold g}{\bold A \bold A\T}
	+ \bold m
\label{eqn:newdirection}
\end{equation}

\par
The search direction (\ref{eqn:newdirection}) 
shows a positive-definite operator scaling the gradient.
Each component of any gradient vector is independent of each other.
All independently point a direction for descent.
Obviously, each can be scaled by any positive number.
Now we have found that we can also scale a gradient vector by
a positive definite matrix and we can still expect
the conjugate-direction algorithm to descend, as always,
to the ``exact'' answer in a finite number of steps.
This is because modifying the search direction with
$ \bold A^{-1} (\bold A^{-1})\T$ is equivalent to solving
a conjugate-gradient problem in $\bold p$.

%\subsubsection{Implications for better preconditioning}
%\par
%The search direction (\ref{eqn:newdirection})
%brings us both good news and bad news.
%First the bad news.
%Equation (\ref{eqn:newdirection}) looks like something new
%but it will probably turn out to be equivalent to all the
%preconditioning work we have done with the operator $\bold A^{-1}$
%being polynomial division.
%Actually, we do polynomial division
%followed by truncation of the infinite series.
%Because of the truncation we really do not properly construct
%$ \bold A^{-1} (\bold A^{-1})\T$, and for this we can expect
%some difficulty at one boundary.

%\par
%The good news is that we now see new preconditioning opportunities.
%Formerly we began from a preconditioning 
%operator $\bold A^{-1}$ but now we see that we really only
%need its spectrum $\bold A^{-1} (\bold A^{-1})\T$.
%That might not be any real advantage since our basic representation
%of a positive definite matrix is any matrix times its transpose.
%Anyway, we can conclude that the operator $\bold A$
%may be chosen as any cascade of weighting and filtering operators.
%
%\par
%We are also reminded that what we really need
%is not simply $\bold A^{-1} (\bold A^{-1})\T$
%but $(\bold F\T\bold F+\bold A\T\bold A)^{-1}$.
%Likewise we imagine this being built of
%any cascade of weighting and filtering operators
%followed by the adjoint.

\subsection{The meaning of the preconditioning variable $\bold p$}

To accelerate convergence of iterative methods we often change variables.
The model-styling regression $\bold 0 \approx \epsilon \bold A \bold m$
is changed to $\bold 0 \approx \epsilon \bold p$.
Experience shows, however, that the variable $\bold p$ is often more interesting
to look at than the model $\bold m$.
Why should a new variable introduced for computational convenience
turn out to have more interpretive value?
There is a little theory underlying this.  Begin from
\begin{eqnarray}
\bold 0 &\approx& \bold W (\bold F \bold m -\bold d)
\\
\bold 0 &\approx& \epsilon \bold A \bold m
\end{eqnarray}
Introduce the preconditioning variable $\bold p$.
\begin{eqnarray}
\bold 0 &\approx& \bold W (\bold F \bold A^{-1}\bold p -\bold d)
\\
\bold 0 &\approx& \epsilon \bold p
\end{eqnarray}
Rewrite this as a single regression
\begin{equation}
\label{eqn:singlereg}
\bold 0 
\quad\approx\quad
\left[
\begin{array}{c}
\bold r_d \\
\bold r_m
\end{array}
\right]
\quad=\quad
\left[
\begin{array}{c}
\bold W\bold F\bold A^{-1} \\
\epsilon \bold I
\end{array}
\right]
\bold p
\quad - \quad
\left[
\begin{array}{c}
\bold d \\
\bold 0
\end{array}
\right]
\end{equation}
In Chapter \ref{paper:lsq} we learned the least squares solution
is when the residual is orthogonal to the fitting functions.
The fitting functions are the columns of the matrix or the rows of its transpose.
Thus we simply multiply the regression
(\ref{eqn:singlereg})
by the adjoint operator and replace the $\approx$ by $=$.
Thus
\begin{equation}
\label{eqn:fight}
\bold 0 \quad=\quad (\bold W\bold F\bold A^{-1})\T\bold r_d + \epsilon \bold r_m 
\end{equation}
Equation (\ref{eqn:fight}) tells us at the best solution to the regression
there is a fight between the data space residual and the model space residual.
It's a battle between our preconceived statistical model
expressed in our model styling
and the model wanted by the data.
Except for the scale factor $\epsilon$,
the model space residual $\bold r_m$ is the preconditioning variable $\bold p$.
That's why the variable $\bold p$ is interesting to inspect and interpret.
The variable $\bold p$ is not simply a computational convenience.
Its size measures (in model space)
the conflict of our acquired data with our preconceived theory.
It points to locations of interest.
\par
\boxit{
The preconditioning variable $\bold p$ is not simply a computational convenience.
Its size measures (in model space)
the conflict of our acquired data with our preconceived theory
expressed by our model styling.
It points to locations of interest.
}

\par
If I were young and energetic like you
I would write a new basic tool for optimization.
Instead of scanning only the space of the gradient and previous step,
it would scan also over the ``smart'' direction.  
This should offer the benefit of preconditioning
the regularization at early iterations
while offering more assured fitting data at late iterations.
The improved 
module for \texttt{cgstep}
%\vpageref{/prog:cgstep}
would need to solve a $3\times 3$ matrix.
%XXX



\section{NULL SPACE AND INTERVAL VELOCITY}
A bread-and-butter problem in seismology is building the velocity
as a function of depth (or vertical travel time)
starting from certain measurements.
The measurements are described elsewhere (BEI for example).
They amount to measuring the integral of the velocity squared
from the surface down to the reflector.
It is known as the RMS (root-mean-square) velocity.
Although good quality echoes may arrive often,
they rarely arrive continuously for all depths.
Good information is interspersed unpredictably with poor information.
Luckily we can
also estimate
the data quality by the ``coherency'' or the
``stack energy''.
In summary, what we get from observations and preprocessing
are two functions of travel-time depth,
(1) the integrated (from the surface) squared velocity, and
(2) a measure of the quality of the integrated velocity measurement.
Some definitions:
\begin{description}
\item  [$\bold d$]
is a data vector whose components range over the vertical
traveltime depth $\tau$,
and whose component values contain the scaled RMS velocity squared
$\tau v_{\rm RMS}^2/\Delta \tau $
where
$\tau /\Delta \tau $ is the index on the time axis.
\item [$\bold W$]
is a diagonal matrix along which we lay the given measure
of data quality.  We will use it as a weighting function.
\item  [$\bold C$]
is the matrix of causal integration, a lower triangular matrix of ones.
\item  [$\bold D$]
is the matrix of causal differentiation, namely, $\bold D=\bold C^{-1}$.
\item [$\bold u$]
is a vector whose components range over the vertical
traveltime depth $\tau$,
and whose component values contain the interval velocity squared
$v_{\rm interval}^2 $.
\end{description}
From these definitions,
under the assumption of a stratified earth with horizontal reflectors
(and no multiple reflections)
the theoretical (squared) interval velocities
enable us to define the theoretical (squared) RMS velocities by
\begin{equation}
\bold C\bold u \eq \bold d 
\end{equation}
With imperfect data, our data fitting goal is to minimize the residual
\begin{equation}
\bold 0
\quad\approx\quad
\bold W
\left[
\bold C\bold u
-
\bold d
\right]
\end{equation}
\par
To find the interval velocity
where there is no data (where the stack power theoretically vanishes)
we have the ``model damping'' goal to minimize
the wiggliness $\bold p$
of the squared interval velocity $\bold u$.
\begin{equation}
\bold 0
\quad\approx\quad
\bold D \bold u \eq \bold p
\end{equation}
\par
We precondition these two goals
by changing the optimization variable from
interval velocity squared
$\bold u$ to its wiggliness $\bold p$.
Substituting $\bold u=\bold C\bold p$ gives the two goals
expressed as a function of wiggliness $\bold p$.
\begin{eqnarray}
\label{eqn:model}
\bold 0
&\approx&
\bold W
\left[
\bold C^2\bold p
-
\bold d
\right]
\\
\bold 0
&\approx&
\epsilon \; \bold p
\end{eqnarray}
\subsection{Balancing good data with bad}
\inputdir{bob}
Choosing the size of $\bold \epsilon$ chooses
the stiffness of the curve that connects regions of good data.
Our first test cases gave solutions
that we interpreted to be
too stiff at early times and too flexible at later times.
This leads to two possible ways to deal with the problem.
One way modifies the model shaping
and the other modifies the data fitting.
The program below weakens the data fitting weight with time.
This has the same effect as stiffening the model shaping with time.

\plot{clapp}{width=6in,height=3in}{
  Raw CMP gather (left),
  Semblance scan (middle),
  and semblance value used for weighting function (right).
}

\plot{stiff}{width=6in,height=3in}{
  Observed RMS velocity
  and that predicted by a stiff model with
  $\epsilon=4$.
}

\plot{flex}{width=6in,height=3in}{
  Observed RMS velocity
  and that predicted by a flexible model with
  $\epsilon=.25$
  (Clapp)
}

%\subsection{Bigsolver}
%The regression (\ref{eqn:model}) includes a weighting function,
%so we need yet another solver module
%very much like the regularized and preconditioned solvers
%that we used earlier.
%The idea is essentially the same.
%Instead of preparing such a solver here,
%I refer you to the end of the book for
%\texttt{solver\_mod} \vpageref{lst:bigsolver},
%a big solver that incorporates everything
%that we need in the book.
%Hopefully we will not need to look at solvers again for a while.
%Module \texttt{vrms2int} \vpageref{lst:vrms2int}
%was written by Bob Clapp to get the results
%in Figures \ref{fig:clapp} to \ref{fig:flex}.
%Notice that he is using
%\texttt{solver\_mod} \vpageref{lst:bigsolver}.

\begin{comment}
\moddex{vrms2int}{Converting RMS to interval velocity}
\end{comment}

\subsection{Lateral variations}
The analysis above appears one dimensional in depth.
Conventional interval velocity estimation builds a velocity-depth model
independently at each lateral location.
Here we have a logical path for combining measurements
from various lateral locations.
We can change the regularization
to something like $\bold 0\approx \nabla\bold u$.
Instead of merely minimizing the vertical gradient of velocity
we minimize its spatial gradient.
Luckily we have preconditioning and the helix to speed the solution.
\par
%Likewise the construction of the data quality screen $\bold G$
%would naturally involve the full three-dimensional setting.

\subsection{Blocky models}
\inputdir{.}

Sometimes we seek a velocity model that increases smoothly
with depth through our scattered
measurements of good-quality RMS velocities.
Other times, we seek a blocky model.
(Where seismic data is poor,
a well log could tell us whether to choose smooth or blocky.)
Here we see an estimation method that can choose the blocky alternative,
or some combination of smooth and blocky.

\par
Consider the five layer model in Figure \ref{fig:rosales}.
Each layer has unit traveltime thickness
(so integration is simply summation).
Let the squared interval velocities be $(a,b,c,d,e)$
with strong reliable reflections at the base of layer $c$ and layer $e$,
and weak, incoherent, ``bad'' reflections at bases of $(a,b,d)$.
Thus we measure $V_c^2$ the RMS velocity squared of the top three layers
and $V_e^2$ that for all five layers.
Since we have no reflection from at the base of the fourth layer,
the velocity in the fourth layer is not measured but a matter for choice.
In a smooth linear fit we would want $d=(c+e)/2$.
In a blocky fit we would want $d=e$.

\sideplot{rosales}{width=1.5in}{
  A layered earth model.
  The layer interfaces cause reflections.
  Each layer has a constant velocity in its interior.
}


\par
Our screen for good reflections looks like $(0,0,1,0,1)$
and our screen for bad ones looks like the complement $(1,1,0,1,0)$.
We put these screens on the diagonals of diagonal matrices
$\bold G$ and $\bold B$.
Our fitting goals are:
\begin{eqnarray}
3V_c^2 &\approx& a+b+c
\\
5V_e^2 &\approx& a+b+c+d+e
\\
u_0 &\approx& a
\\
0 &\approx& -a+b
\\
0 &\approx& -b+c
\\
0 &\approx& -c+d
\label{eqn:block}
\\
0 &\approx& -d+e
\end{eqnarray}
For the blocky solution, we do not want the fitting goal (\ref{eqn:block}).
Further explanations await completion of examples.

%We can remove it by multiplying the model goals by a diagonal-matrix
%badpass screen $\bold B$ 
%eliminating the goal $0 \approx -c+d$.
%In abstract, our fitting goals become
%\begin{eqnarray}
%\bold 0 &\approx& \bold r \eq \bold C \bold u  - \bold d
%\\
%\bold 0 &\approx& \bold p \eq \bold B\bold D\bold u - \bold u_0
%\end{eqnarray}
%where $\bold u_0$ is a zero vector with a top component of $u_0$.
%Since $\bold B$ is not invertable,
%we cannot backsolve the preconditioned variable $\bold p$ for
%the squared interval velocity
%$\bold u= \bold D^{-1}\bold B^{-1}(\bold p + \bold u_0)$.
%Instead, we use $\bold G$ for $\bold B^{-1}$
%thus redefining
%the implicit relationship for $\bold u$.
%\begin{equation}
%\bold u \eq \bold u_0 + \bold D^{-1}\bold G\bold p
%\label{eqn:ufromp}
%\end{equation}
%where $\bold G$ is the goodpass screen.
%Since $\bold D^{-1}=\bold C$ the fitting goals become
%
%
%\begin{equation}
%\begin{array}{lll}
%\bold 0 &\approx& \bold r \eq \bold C^2\bold G \bold p
%+\bold C \bold u_0 
%-\bold d
%\\
%\bold 0 &\approx& \bold p
%\end{array}
%\label{eqn:logical}
%\end{equation}
%After fitting with $\bold p$,
%we define the squared interval-velocity
%$\bold u$ using (\ref{eqn:ufromp}).
%
%\par
%The formulation
%(\ref{eqn:logical})
%is so logical that we might have guessed it:
%The goal $\bold 0 \approx \bold p $ says that $\bold p$ is mostly zero.
%What emerges from $\bold G$ is a sprinkling of impulses.
%Then $\bold C^2$ converts the pulses to ramp functions
%(zero until a certain place, then growing linearly)
%which are used to fit the data (integrated velocity).
%Differentiating the data-fitting ramps
%converts them to the desired blocks of constant velocity.
%One iteration is required for each impulse.
%
%\par
%Choosing $\bold G$ to be an identity $\bold I$ gives smooth velocity models,
%such as caused by the increasing consolidation of the rocks with depth.
%Choosing the screen $\bold G$ to have a sprinkling of pass locations
%picks the boundaries of blocks of constant velocity.
%The choice can be made by people with subjective criteria (like geologists)
%or we can assist by using the data itself
%in various ways to select our degree of preference between
%the blocky and smooth models.
%For example,
%we could put seismic coherency or amplitude
%on the goodpass diagonal matrix $\bold G$.
%Clearly, much remains to be gained from experience.
%



%\begin{notforlecture}



\section{INVERSE LINEAR INTERPOLATION}
\inputdir{sep94}
\sideplot{data}{width=3in,height=1.5in}{
  The input data are irregularly sampled.
}
The first example is a simple synthetic test for 1-D inverse
interpolation. The input data were randomly subsampled (with
decreasing density) from a sinusoid (Figure \ref{fig:data}). The
forward operator $\bold L$ in this case is linear interpolation. We seek
a regularly sampled model that could predict the data with a
forward linear interpolation. Sparse irregular distribution of the
input data makes the regularization enforcement a necessity.
I applied convolution with the simple $(1,-1)$
difference filter as the operator $\bold D$ that forces model continuity
(the first-order spline).
An appropriate preconditioner $\bold S$ in this
case is recursive causal integration. 

%Figures \ref{fig:im1} and
%\ref{fig:fm1} show the results of inverse interpolation after
%exhaustive 300 iterations of the conjugate-direction method.
%As a result of using the causal integration for preconditioning,
%the rightmost part of the model in the data-space case stays at a
%constant level instead of decreasing to zero. If we specifically
%wanted a zero-value boundary condition, it wouldn't be difficult to
%implement it by adding a zero-value data point at the boundary.

%\sideplot{im1}{width=3in,height=1.5in}{ Estimation of a continuous
%  function b regularization. The regularization operator $\bold A$ is
%  the derivative operator (convolution with $(1,-1)$).}

%\sideplot{fm1}{width=3in,height=1.5in}{Estimation of a continuous
%  function by preconditioning model regularization. The
%  preconditioning operator $\bold P$ is causal integration.}

\plot{conv}{width=6in,height=7in}{Convergence history of inverse
  linear interpolation. Left: regularization, right: preconditioning.
  The regularization operator $\bold A$ is
  the derivative operator (convolution with $(1,-1)$. The
  preconditioning operator $\bold S$ is causal integration.}

%\plot{conv2}{width=6in,height=7in}{Convergence history of inverse
%  linear interpolation. Left: regularization, right: preconditioning.
%  The regularization operator $\bold A$ is
%  the second derivative operator (convolution with $(1,-2,1)$. The
%  preconditioning operator $\bold P$ is the corresponding inverse filtering.}

As expected,
preconditioning provides a much faster rate of convergence.
Since iteration to the exact solution
is never achieved in large-scale problems,
the results of iterative optimization may turn out quite differently.
Bill Harlan points out that the two goals
in (\ref{eq:main}) conflict with each other:
the first one enforces ``details'' in the model,
while the second one tries to smooth them out.
Typically, regularized optimization creates
a complicated model at early iterations.
At first, the data fitting goal (\ref{eq:main}) plays a more important role.
Later, the regularization goal (\ref{eq:main}) comes into play
and simplifies (smooths) the model as much as needed.
Preconditioning acts differently.
The very first iterations create a simplified (smooth) model.
Later, the data fitting goal adds more details into the model.
If we stop the iterative process early,
we end up with an insufficiently complex model,
not in an insufficiently simplified one.
Figure \ref{fig:conv} provides a clear illustration of Harlan's observation.

\par
Figure \ref{fig:schwab1}
measures the rate of convergence by the model residual,
which is a distance from the current model to the final solution.
It shows that preconditioning saves many iterations.
Since the cost of each iteration for each method is roughly equal,
the efficiency of preconditioning is evident.
\sideplot{schwab1}{width=2.4in}{
  Convergence of the iterative optimization,
  measured in terms of the model residual.
  The ``p'' points stand for preconditioning;
  the ``r'' points,
  regularization.
}

\begin{comment}
\par
The module \texttt{invint2} \vpageref{lst:invint2}
invokes the solvers to make
Figures \ref{fig:conv}
and
\ref{fig:schwab1}.
We use convolution with
\texttt{helicon} \vpageref{lst:helicon}
for the regularization
and we use deconvolution with
\texttt{polydiv} \vpageref{lst:polydiv}
for the preconditioning.
The code looks fairly straightforward except for
the oxymoron
\texttt{known=aa\%mis}.

\moddex{invint2}{Inverse linear interpolation}        
\end{comment}

%\par
%This example suggests that the philosophy of image creation by
%optimization has a dual orthonormality: First, Gauss (and common
%sense) tells us that the data residuals should be roughly equal in
%size.  Likewise in Fourier space they should be roughly equal in size,
%which means they should be roughly white, i.e. orthonormal.  (I use
%the word ``orthonormal'' because white means the autocorrelation is an
%impulse, which means the signal is statistically orthogonal to shifted
%versions of itself.)  Second, to speed convergence of iterative
%methods, we need a whiteness, another othonormality, in the solution.
%The map image, the physical function that we seek, might not be itself
%white, so we should solve first for another variable, the whitened map
%image, and as a final step, transform it to the ``natural colored''
%map.
%
%\par
%Often geophysicists create a preconditioning matrix $\bold B$ by
%inventing columns that ``look like'' the solutions that they seek.
%Then the space $\bold x$ has many fewer components than the space of
%$\bold m$.  This approach is touted as a way of introducing geological
%and geophysical prior information into the solution.  Indeed, it
%strongly imposes the form of the solution.  Perhaps this approach
%deserves the diminutive term ``curve fitting'' instead of the
%grandiloquent ``geophysical inverse theory.''  Our preferred approach
%is not to invent the columns of the preconditioning matrix, but to
%estimate the prediction-error filter of the model and use its inverse.
%



\section{EMPTY BINS AND PRECONDITIONING}
There are at least three ways to fill empty bins.
Two require a roughening operator $\bold A$ while
the third requires a smoothing operator which
(for comparison purposes) we denote $\bold A^{-1}$.
The three methods are generally equivalent
though they differ in significant details.

\par
The original way in
Chapter \ref{paper:iin} is to
restore missing data
by ensuring that the restored data,
after specified filtering,
has minimum energy, say
$\bold A\bold m\approx \bold 0$.
Introduce the selection mask operator $\bold K$, 
a diagonal matrix with
ones on the known data and zeros elsewhere
(on the missing data).
Thus
$ \bold 0 \approx \bold A(\bold I-\bold K+\bold K)\bold m $ or
\begin{equation}
\bold 0 \quad\approx\quad
\bold A (\bold I-\bold K) \bold m
\ +\ 
\bold A \bold m_k\;,
\label{eqn:style0}
\end{equation}
where we define $\bold m_k$ to be the data
with missing values set to zero by 
$\bold m_k=\bold K\bold m$.

\par
A second way to find missing data is with the set of goals
\begin{equation}
\begin{array}{rrrrr}
\bold 0 & \approx &          \bold K \bold m  & - & \bold m_k \\
\bold 0 & \approx & \epsilon \bold A \bold m  &   &
\end{array}
\label{eqn:style1}
\end{equation}
and take the limit as the scalar $\epsilon \rightarrow 0$.
At that limit, we should have the same result
as equation (\ref{eqn:style0}).

\par
There is an important philosophical difference between
the first method and the second.
The first method strictly honors the known data.
The second method acknowledges that when data misfits
the regularization theory, it might be the fault of the data
so the data need not be strictly honored.
Just what balance is proper falls to the numerical choice of $\epsilon$,
a nontrivial topic.

\par
A third way to find missing data is to precondition
equation (\ref{eqn:style1}),
namely, try the substitution 
$\bold m = \bold A^{-1} \bold p $.
\begin{equation}
\begin{array}{rrrrr}
\bold 0 & \approx & \bold K \bold A^{-1} \bold p  &-& \bold m_k \\
\bold 0 & \approx & \epsilon             \bold p  & &
\end{array}
\label{eqn:style2}
\end{equation}
There is no simple way of knowing beforehand
what is the best value of $\epsilon$.
Practitioners like to see solutions for various values of $\epsilon$.
Of course that can cost a lot of computational effort.
Practical exploratory data analysis is more pragmatic.
Without a simple clear theoretical basis,
analysts generally begin from $\bold p=0$
and abandon the fitting goal $\epsilon \bold I \bold p\approx 0$.
Implicitly, they take $\epsilon=0$.
Then they examine the solution as a function of iteration,
imagining that the solution at larger iterations
corresponds to smaller $\epsilon$.
There is an eigenvector analysis
indicating some kind of basis for this approach,
but I believe there is no firm guidance.

\par
Before we look at coding details for the three methods
of filling the empty bins,
we'll compare results of trying all three methods.
For the roughening operator $\bold A$,
we'll take the helix derivative $\bold H$.
This is logically equivalent to roughening with the gradient $\bold \nabla$
because the (negative) laplacian operator is
$\bold\nabla\T\bold\nabla = \bold H\T\bold H $.

%Later, in Chapter
%\ref{mda/paper:mda}
%we'll find a smarter roughening operator $\bold A$ that gives better results.

\subsection{Faking the epsilon}

I'll tell a common story that should horrify those interested in theory
(under 23 years old), and delight those wanting to get real work done
(over 23 years old).
Suppose you have a problem where
model space is an image, maybe $1000\times 1000$,
while application of the operator $\bold F$ takes about an hour.
A common example in petroleum prospecting is ``wave equation migration''.
The older generation of workers thus did a migration
(single application of an adjoint operator)
and finished their task in an hour.
You, however, have read this book or otherwise learned about inversion
(iteratively modeling for an increasingly better fitting model).
Twelve iterations (12 applications of both $\bold F\T$ and $\bold F$)
requires 24 hours.
Theoretically you should be doing a million iterations
($10^6$, one for each point in model space),
but you are hoping 12 will be enough. (Good luck!)
\par
Oops! You also need to choose a numerical value for $\epsilon$.
Since we have no theory for that choice, perhaps we should scan over 7 possible values,
or maybe 30.  Now the job takes a week or a month.
That's really bad news because
there typically are other more important and more difficult problems
we also need to be working out
(such as updating the seismic velocity model).
Any trick, honest or sly, would be nice to know.
I'll tell you a trick that is very widely used.
\par
Imagine we had plenty of computer time.
For small $\epsilon$ we have a smaller residual of data fitting but
a larger residual in the model styling goal.
Conversely, for large $\epsilon$, we may fit the data poorly,
but the model styling goal is well honored.
Now please carefully examine the preconditioned steps in Figure \ref{fig:conv}.
At early iterations we see the solution is piece-wise lines (good)
but many data points are poorly fit (bad).
This is like large $\epsilon$.
After more iterations the solution fits the data increasingly well.
This is like small $\epsilon$.
Are you ready to guess what pragmatic people do when they are pressured to get results?

\par
Many (most?) practical studies are done ignoring (abandoning)
the model styling regression (second fitting regression below):
$$
\begin{array}{lll}
\label{eq:main}
\bold 0 &\approx & \bold F \bold A^{-1} \bold p \ -\   \bold d \\
\bold 0 &\approx &           \epsilon   \bold p 
\end{array}
$$
Nice to be rid of $\epsilon$!
The pragmatic person iterates the data fitting regression only,
watches the solution as a function of iteration,
and stops when tired,
or (more hopefully) stops at the iteration that is subjectively most pleasing.


\subsection{SEABEAM: Filling the empty bins with a laplacian}
\sx{seabeam}
\inputdir{seab}

Figure~\ref{fig:seabin} shows a day's worth of data\footnote{
        I'd like to thank Alistair Harding
        for this interesting data set
        named April 18.
        \sx{Harding, Alistair}
        }
collected at sea by \bx{SeaBeam},
an apparatus for measuring water depth both
directly under a ship, and somewhat off to the sides of the ship's track.
The data is measurements of depth $h(x,y)$ at miscellaneous locations
in the $(x,y)$-plane.
\sideplot{seabin}{width=3.0in,height=3.0in}{
  Depth of the ocean under ship tracks.
  Empty bins are displayed with an average depth $\bar h$.
}
The locations are scattered about,
according to various aspects
of the ship's navigation and the geometry of the SeaBeam sonic antenna.
Figure \ref{fig:seabin} was made by binning with
\texttt{bin2()} \vpageref{lst:bin2}
and equation (\ref{eqn:dpbin}).
The spatial spectra of the noise in the data
could be estimated where tracks cross over themselves.
This might be worth while, but we do not pursue it now.

\inputdir{seabeam}

\par
Here we focus on the empty mesh locations where no data is recorded
(displayed with the value of the mean depth $\bar h$).
These empty bins were filled with
module \texttt{mis2} \vpageref{lst:mis2}.
Results are in Figure \ref{fig:prcfill}.
\plot{prcfill}{width=6in,height=8in}{
  The ocean bottom restoring missing data with a helix derivative.
}
In Figure \ref{fig:prcfill} the left column results from 20 iterations
while the right column results from 100 iterations.
\par
The top row in Figure \ref{fig:prcfill} shows that more iterations
spreads information further into the region of missing data.
\par
It turned out that the original method strictly honoring known data
gave results so similar to the second method (regularizing)
that the plots could not be visually distinguished.
The middle row in Figure \ref{fig:prcfill} therefore shows
the difference in the result of the two methods.
We see an outline of the transition between known and unknown regions.
Obviously, the missing data is pulling known data towards zero.

\par
The bottom row in Figure \ref{fig:prcfill} shows that preconditioning
spreads information to great distances much quicker
but early iterations make little effort to honor the data.
(Even though these results are for $\epsilon=0$.)
Later iterations make little change at long distance
but begin to restore sharp details on the small features
of the known topography.

\par
What if we can only afford 100 iterations?
Perhaps we should first do 50 iterations with
preconditioning to develop the remote part of the solution and
then do 50 iterations by one of the other two methods to be sure
we attended to the details near the known data.
A more unified approach (not yet tried, as far as I know)
would be to unify the techniques.
The conjugate direction method searches two directions,
the gradient and the previous step.
We could add a third direction,
the smart direction of equation (\ref{eqn:newdirection}).
Instead of having a $2\times 2$ matrix
solution like equation (\ref{eqn:twobytwosln}) for two distances,
we would need to solve a $3\times 3$ matrix for three.

\par
Figure \ref{fig:prcfill} has a few artifacts connected with the
use of the helix derivative.
Examine equation (\ref{eqn:lapfac}) to notice the shape of the helix derivative.
In principle, it is infinitely long in the horizontal axis
in both equation (\ref{eqn:lapfac}) and Figure \ref{fig:prcfill}.
In practice, it is truncated.  The truncation is visible as bands
along the sides of Figure \ref{fig:prcfill}.

\par
As a practical matter, no one would use the first two bin filling methods
with helix derivative for the roughener
because it is theoretically equivalent to the gradient operator
$\bold \nabla$ which has many fewer coefficients.
Later, in Chapter
\ref{paper:mda}
we'll find a much smarter roughening operator $\bold A$
called the Prediction Error Filter (PEF)
which gives better results.





%The top row in Figure \ref{fig:prcfill} shows that more iterations
%The second 
%After many iterations, both regularization and preconditioning
%lead us to the same result.
%After a small number of iterations, we see that
%regularization has filled the small holes
%but it has not reached out far away from the known data.
%With preconditioning, it is the opposite.


\subsection{Three codes for inverse masking}
The selection (or masking) operator $\bold K$ is
implemented in
\texttt{mask()} \vpageref{lst:mask}.
\opdex{mask}{copy under mask}{47}{52}{api/c}
%The inverting of the mask operator proceeds much as
%we inverted the linear interpolation operator with
%module \texttt{invint2} \vpageref{lst:invint2}.
%The main difference is we swap the selection operator
%for the linear interpolation operator.
%(Philosophically, selection is like binning which is
%like nearest-neighbor interpolation.)

All the results shown in Figure \ref{fig:prcfill}
were created with the module
\texttt{mis2} \vpageref{lst:mis2}.
Code locations with \texttt{style=0,1,2}
correspond to the fitting goals
(\ref{eqn:style0}), (\ref{eqn:style1}), (\ref{eqn:style2}).
\moddex{mis2}{Missing data interpolation with and without preconditioning}{24}{54}{user/gee} %XX
\par

%It is instructive to compare
%\texttt{mis2} \vpageref{lst:mis2} with
%\texttt{invint2} \vpageref{lst:invint2}.
%Both are essentially filling empty regions
%consistant with prior knowledge at particular locations
%and minimizing energy of the filtered field.
%Both use the helix and can be used in $N$-dimensional space.


\section{THEORY OF UNDERDETERMINED LEAST-SQUARES}
Construct theoretical data with
\begin{equation}
         \bold d \eq \bold F \bold m
        \label{eqn:theordata}
\end{equation}
Assume there are fewer data points than model points
and that the matrix $\bold F \bold F\T$ is invertible.
From the theoretical data we estimate a model $\bold m_0$ with
\begin{equation}
         \bold m_0 \eq \bold F\T (\bold F \bold F\T)^{-1} \bold d
        \label{eqn:underdetest}
\end{equation}
To verify the validity of the estimate,
insert the estimate (\ref{eqn:underdetest}) into the
data modeling equation (\ref{eqn:theordata}) and notice
that the estimate $\bold m_0$ predicts the correct data.
Notice that equation
(\ref{eqn:underdetest}) is not the same
as equation (\ref{eqn:sln}) which we derived much earlier.
What's the difference?
The central issue is which matrix of
$\bold F \bold F\T$ and
$\bold F\T \bold F$ actually has an inverse.
If $\bold F$ is a rectangular matrix,
then it is certain that one of the two is not invertible.
(There are plenty of real cases where neither matrix is invertible.
That's one reason we use iterative solvers.)
Here we are dealing with the case with more model points than data points.

\par
Now we will show that of all possible models $\bold m$ that
predict the correct data, $\bold m_0$ has the least energy.
(I'd like to thank Sergey Fomel for this clear and simple proof
that does {\em  not} use Lagrange multipliers.)
First split (\ref{eqn:underdetest}) into an intermediate
result $\bold d_0$ and final result:
\begin{eqnarray}
        \bold d_0 &=& (\bold F \bold F\T)^{-1} \bold d
	\\
        \bold m_0 &=& \bold F\T \bold d_0
\end{eqnarray}
Consider another model ($\bold x$ not equal to zero)
\begin{equation}
        \bold m \eq \bold m_0 + \bold x
\end{equation}
which fits the theoretical data
$\bold d = \bold F(\bold m_0+\bold x)$.
Since
$\bold d=\bold F\bold m_0$,
we see
that $\bold x$ is a null space vector.
\begin{equation}
        \bold F \bold x \eq \bold 0
\end{equation}
First we see that $\bold m_0$ is orthogonal to $\bold x$ because
\begin{equation}
         \bold m_0' \bold x \eq
         (\bold F\T \bold d_0)\T \bold x \eq
         \bold d_0' \bold F \bold x \eq
         \bold d_0' \bold 0 \eq 0
	\label{eqn:dzero}
\end{equation}
Therefore,
\begin{equation}
\bold m\T \bold m \eq
\bold m_0' \bold m_0 + \bold x\T\bold x + 2 \bold x\T \bold m_0 \eq
\bold m_0' \bold m_0 + \bold x\T\bold x \quad\ge\quad \bold m_0' \bold m_0 
\end{equation}
so adding null space to $\bold m_0$ can only increase its energy.
In summary,
the solution
$\bold m_0 = \bold F\T (\bold F \bold F\T)^{-1} \bold d$
has less energy than any other model that satisfies the data.

\par
Not only does the theoretical solution
$\bold m_0 = \bold F\T (\bold F \bold F\T)^{-1} \bold d$
have minimum energy,
but the result of iterative descent will too,
provided that we begin iterations from $\bold m_0=0$ or any $\bold m_0$
with no null-space component.
In (\ref{eqn:dzero}) we see that the 
orthogonality $\bold m_0'\bold x=0$
does not arise because $\bold d_0$ has any particular value.
It arises because $\bold m_0$ is of the form $\bold F\T\bold d_0$.
Gradient methods contribute $\Delta\bold m =\bold F\T \bold r$
which is of the required form.

%\section{VIRTUAL-RESIDUAL PRECONDITIONING}
%Sergey Fomel has developed a wholly different
%approach to preconditioning to that described above.
%He calls his method the ``model-extension'' method.
%I call it the ``virtual-residual'' method.
%Both names are suggestive of the method.
%\par
%We begin with the usual definition
%of preconditioned variables $\bold m =\bold S\bold p$ and
%the opposite of our usual sign convention for the residual.
%\begin{eqnarray}
%-\epsilon \ \bold r &=&  \bold F         \bold m - \bold d \\
%-\epsilon \ \bold r &=&  \bold F \bold S \bold p - \bold d
%\label{eq:r}
%\end{eqnarray}
%which we arrange with a column vector of unknowns that includes
%not only $\bold p$ but also $\bold r$.
%\begin{equation}
%\bold 0
%\quad\approx\quad \hat {\bold r}
%\eq 
%\left[ \bold F \bold S \quad \epsilon \ \bold I \right]
%\
%\left[
%\begin{array}{c}
%\bold p \\
%\bold r
%\end{array}
%\right]
%\ - \  \bold d
%\end{equation}
%The interesting thing is that we can use our familiar
%conjugate-gradient programs with this system of equations.
%We only need to be careful to distinguish the residual in the bottom half
%of our solution vector,
%from the virtual residual $\hat {\bold r}$
%(which should vanish identically)
%in the iterative fitting problem.
%The fitting begins with the initializations:
%\begin{equation}
%\hat{\bold p}_0  \eq \left[
%\begin{array}{c}
%\bold p_0 \\
%\bold 0
%\end{array}
%\right]
%\end{equation}
%and
%\begin{equation}
%\hat{\bold r}_0  \eq
%      \bold F\bold S \bold p_0 -\bold d \eq
%      \bold F        \bold m_0 -\bold d
%\end{equation}
%As the iteration begins we have gradients
%of the two parts of the model
%\begin{eqnarray}
%\bold g_m &=& \bold S\T \bold F\T \hat{\bold r} \\
%\bold g_d &=& \epsilon \        \hat{\bold r}
%\end{eqnarray}
%which imply a perturbation in the theoretically zero residual
%\begin{equation}
%\Delta\hat{\bold r}  \eq
%      \bold F\bold S \bold g_m + \epsilon\; \bold g_d
%\end{equation}
%Then step sizes and steps are determined
%as usual for conjugate-direction fitting.
%\par
%The preconditioning module \texttt{vr\_solver}
%%\vpageref{lst:vrsolver}
%takes three functions as its arguments.
%Functions \texttt{oper} and \texttt{prec} correspond to the linear
%operators $\bold F$ and $\bold S$.
%Function \texttt{solv} implements one step of an optimization descent.
%Its arguments are a logical parameter \texttt{forget},
%which controls a conditional restarting of the optimization,
%the current effective model \texttt{x0},
%the gradient vector \texttt{g}, the data residual vector \texttt{rr}, and
%the conjugate gradient vector \texttt{gg}.
%Subroutine \texttt{solver\_vr} constructs
%the effective model vector \texttt{x},
%which consists of the model-space part \texttt{xm}
%and the data-space part \texttt{xd}.
%Similarly, the effective gradient vector \texttt{g}
%is split into the the model-space part \texttt{gm}
%and the data-space part \texttt{gd}.
%\par
%For brevity I am omitting the code
%for the virtual-residual solver \texttt{vrsolver}
%which is in the library.
%It follows the same pattern as
%\texttt{prec\_solver} \vpageref{lst:precsolver}.
%%\moddex{vrsolver}{Virtual-residual solver}

\section{SCALING THE ADJOINT}

First I remind you of a rarely used little bit of mathematical notation.
Given a vector $\bold m$ with components $(m_1,m_2,m_3)$,
the notation $ {\bf diag\ }\bold m$ means
\begin{equation}
 {\bf diag\ }\bold m \eq
	\left[
 	\begin{array}{ccc}
		m_1 &  0 & 0 
		\\
		0   &m_2 & 0
		\\
		0   &  0 & m_3
	\end{array}
	\right]
\end{equation}
\par
Given the usual linearized fitting goal between
data space and model space, $ \bold d \approx \bold F \bold m$,
the simplest image of the model space results from
application of the adjoint operator $ \hat{\bold m} = \bold F\T \bold d$.
Unless $\bold F$ has no physical units, however,
the physical units of $\hat{\bold m}$ do not match those of $\bold m$,
so we need a scaling factor.
The theoretical solution
$\bold m_{\rm theor} = (\bold F\T\bold F)^{-1}\bold F\T\bold d$
tells us that the scaling units should be those of $(\bold F\T\bold F)^{-1}$.
We are going to approximate $(\bold F\T\bold F)^{-1}$ by a diagonal matrix
$\bold W^2$ with the correct units so
$\hat{\bold m} = \bold W^2 \bold F\T\bold d$.

\par
What we use for $\bold W$ will be a guess, simply a guess.
If it works better than nothing, we'll be happy,
and if it doesn't we'll forget about it.
Experience shows it is a good idea to try.
Common sense tells us to insist that all elements of $\bold W^2$ are positive.
$\bold W^2$ is a square matrix of size of model space.
From any vector $\tilde {\bold m}$ in model space with all positive components,
we could guess that $\bold W^2$ be
${\bf diag\ } \tilde {\bold m}$ to any power.
To get the right physical dimensions we choose
$\tilde {\bold m}=\bold 1$, a vector of all ones and choose
\begin{equation}
\bold W^2 \eq
	\frac{ 1 }{ {\bf diag\ } {\bold F\T\bold F\bold 1} }
\label{eqn:adjointwt}
\end{equation}
A problem with the choice
(\ref{eqn:adjointwt}) is that some components might be zero or negative.
Well, we can take the square root of the squares of components
and/or smooth the result.

\par
To go beyond the scaled adjoint we can use $\bold W$ as a \bx{precondition}er.
To use $\bold W$ as a preconditioner
we define implicitly a new set of variables $\bold p$
by the substitution $\bold m=\bold W\bold p$.
Then $\bold d \approx \bold F\bold m=\bold F\bold W\bold p$.
To find $\bold p$ instead of $\bold m$,
we iterate
with the operator $\bold  F\bold W$ instead of with $\bold F$.
As usual, the first step of the iteration is to use the adjoint
of $\bold d\approx \bold F\bold W\bold p$ to form the image
$\hat{\bold p}=(\bold F\bold W)\T\bold d$.
At the end of the iterations,
we convert from  $\bold p$ back to  $\bold m$
with $\bold m=\bold W\bold p$.
The result after the first iteration
$\hat{\bold m}=\bold W\hat{\bold p}=\bold W(\bold F\bold W)\T\bold d=\bold W^2\bold F\T\bold d$
turns out to be the same as scaling.
\par
By (\ref{eqn:adjointwt}), $\bold W$ has physical units inverse to $\bold F$.
Thus the transformation $\bold F\bold W$ has no units
so the $\bold p$ variables have physical units of data space.
Experimentalists might enjoy seeing the 
solution $\bold p$
with its data units more than viewing the solution $\bold m$
with its more theoretical model units.

\par
The theoretical solution for underdetermined systems
         $\bold m =\bold F\T (\bold F \bold F\T)^{-1} \bold d$
suggests
an alternate approach using instead
         $\hat{\bold m} =\bold F\T \bold W_d^2 \bold d$.
This diagonal weighting matrix $\bold W_d^2$ must be drawn
from vectors in data space.
Again I chose a vector of all 1's getting the weight
\begin{equation}
\bold W_d^2 \eq
	\frac{ 1 }{ {\bf diag\ } {\bold F\bold F\T\bold 1}
	}
\label{eqn:datawtadjoint}
\end{equation}

\par
My choice of a vector of 1's is quite arbitrary.
I might as well have chosen a vector of random numbers.
Bill Symes, who suggested this approach to me,
suggests using an observed data vector $\bold d$ for the data space weight,
and $\bold F\T \bold d$ for the model space weight.
This requires an additional step, dividing out the units of the data $\bold d$.

\par
Experience tells me that a broader methodology than all above is needed.
Appropriate scaling is required in both data space and model space.
We need two other weights
$\bold W_m$ and
$\bold W_d$ where
$\hat{\bold m} = \bold W_m \bold F\T\bold W_d \bold d$.

%\par
%I have a useful practical example (stacking in $v(z)$ media)
%in another of my electronic books (BEI),
%where I found both
%$\bold W_m$ and
%$\bold W_d$ by iterative guessing.
%First assume $\bold W_d =\bold I$ and estimate $\bold W_m$ as above.
%Then assume you have the correct $\bold W_m$ and estimate $\bold W_d$ as above.
%Iterate.
%Perhaps some theorist can find a noniterative solution.

\par
I have a useful practical example (stacking in $v(z)$ media)
in another of my electronic books (BEI),
where I found both
$\bold W_m$ and
$\bold W_d$ by iterative guessing.
First assume $\bold W_d =\bold I$ and estimate $\bold W_m$ as above.
Then assume you have the correct $\bold W_m$ and estimate $\bold W_d$ as above.
Iterate.
(Perhaps some theorist can find a noniterative solution.)
I believe this iterative procedure leads us to the best diagonal
pre- and post- multipliers for any operator $\bold F$.  
By this I mean that the modified operator 
$(\bold W_d \bold F \bold W_m)$
is as close to being unitary as we will be able to obtain
with diagonal transformation.
Unitary means it is energy conserving and that the inverse
is simply the conjugate transpose.
\par
What good is it that
$(\bold W_d \bold F \bold W_m)\T
(\bold W_d \bold F \bold W_m) \approx \bold I$?
It gives us the most rapid convergence of least squares problems of the form
\begin{equation}
\label{eqn:genform}
\bold 0 \quad\approx\quad \bold W_d ( \bold F\bold m - \bold d)
\eq               \bold W_d ( \bold F\bold W_m \bold p - \bold d)
\end{equation}
Thus it defines for us the best
diagonal transform to a
preconditioning variable
$\bold p=\bold W_m^{-1}\bold m$ to use during iteration,
and suggests to us what residual weighting function we need to use if
rapid convergence is a high priority.
Suppose we are not satisfied with $\bold W_d$ being the weighting function
for residuals.  Equation~(\ref{eqn:genform}) could still help us speed iteration.
Instead of beginning iteration with $\bold p=\bold 0$,
we could begin from the solution $\bold p$
to the regression~(\ref{eqn:genform}).

\par
The PhD thesis of James Rickett experiments extensively
with data space and model space weighting functions
in the context of seismic velocity estimation.


\section{A FORMAL DEFINITION FOR ADJOINTS}

In mathematics, adjoints are defined a little differently than
we have defined them here
(as matrix transposes).\footnote{
	I would like to thank Albert Tarantola for suggesting this topic.
	}
The mathematician begins by telling us that we cannot simply
form any dot product we want.
We are not allowed to take the dot product of any two vectors
in model space
$\bold m_1 \cdot \bold m_2$ or data space
$\bold d_1 \cdot \bold d_2$.
Instead, we must first transform them to a preferred coordinate system.
Say
$\tilde{\bold m}_1 = \bold M {\bold m}_1$ and
$\tilde{\bold d}_1 = \bold D {\bold d}_1$, etc for other vectors.
We complain we do not know $\bold M$ and $\bold D$.
They reply that we do not really need to know them
but we do need to have the inverses (aack!) of
$\bold M\T\bold M$ and
$\bold D\T\bold D$.
A pre-existing common notation is
$\bold\sigma_m^{-2} = \bold M\T\bold M$ and
$\bold\sigma_d^{-2} = \bold D\T\bold D$.
Now the mathematician buries the mysterious new positive-definite
matrix inverses in the definition of dot product
$<\bold m_1,\bold m_2> = \bold m\T_1 \bold M\T\bold M \bold m_2 = \bold m_1\T \bold \sigma_m^{-2} \bold m_2$
and likewise with
$<\bold d_1,\bold d_2>$.
This suggests a total reorganization of our programs.
Instead of computing
$(\bold m_1\T \bold M\T) (\bold M \bold m_2)$
we could compute $\bold m_1\T (\bold\sigma_m^{-2} \bold m_2)$.
Indeed, this is the ``conventional'' approach.
This definition of dot product would be buried in the solver code.
The other thing that would change would be the search direction
$\Delta \bold m$.
Instead of being the gradient as we have defined it
$\Delta \bold m=\bold L\T\bold r$,
it would be
$\Delta \bold m=\bold\sigma_m^{-2} \bold L\T\bold\sigma_d^{-2}\bold r$.
A mathematician would
{\em define} the adjoint of $\bold L$ to be
$\bold\sigma_m^{-2} \bold L\T\bold\sigma_d^{-2}$.
(Here $\bold L\T$ remains matrix transpose.)
You might notice this approach nicely incorporates
both residual weighting and preconditioning
while yet evading the issue of where we get the matrices
$\sigma_m^2$ and $\sigma_d^2$ or how we invert them.
Fortunately, upcoming chapter \ref{paper:mda}
suggests how,
in image estimation problems,
to obtain sensible estimates of the elusive operators $\bold M$ and $\bold D$.
Parenthetically, modeling calculations in physics and engineering
often use similar mathematics
in which the role of $\bold M\T\bold M$ is not so mysterious.
Kinetic energy is mass times velocity squared.
Mass can play the role of $\bold M\T\bold M$.

\par
So, should we continue to use
$(\bold m_1\T \bold M\T) (\bold M \bold m_2)$
or should we take the conventional route and go with
$\bold m_1\T (\bold\sigma_m^{-2} \bold m_2)$?
One day while benchmarking a wide variety of computers I was shocked
to see some widely differing numerical results.  Now I know why.
Consider adding $10^7$ identical positive floating point numbers, say 1.0's,
in an arithmetic with precision of $10^{-6}$.
After you have added in the first $10^6$ numbers,
the rest will all truncate in the roundoff
and your sum will be wrong by a factor of ten.
If the numbers were added in pairs,
and then the pairs added, etc, there would be no difficulty.
Precision is scary stuff!

\par
It is my understanding and belief that there is nothing wrong
with the approach of this book, in fact,
it seems to have some definite advantages.
While the conventional approach requires one
to compute the adjoint correctly, we do not.
The method of this book
(which I believe is properly called conjugate directions)
has a robustness that, I'm told,
has been superior in some important geophysical applications.
The conventional approach seems to get in trouble when
transpose operators are computed with insufficient precision.



%On the other hand, I can envision applications where
%the conventional approach would be preferable.
%The matrix $\bold M$ is the size of model space times the size of data space
%while the matrix $\bold\sigma_m^{-2}$ is the size of model space squared.
%Clearly, there is the potential for some applications to speed up
%by switching from our approach to the conventional approach.



%\section{ACKNOWLEDGEMENTS}
%Nearly everything I know about null spaces
%I learned from Dave Nichols, Bill Symes, Bill Harlan, and Sergey Fomel.

%\unboldmath


%\clearpage
